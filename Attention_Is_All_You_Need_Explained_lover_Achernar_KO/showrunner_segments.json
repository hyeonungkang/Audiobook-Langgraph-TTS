[
  {
    "segment_id": 1,
    "title": "속삭이는 시작: 낡은 방식과의 작별",
    "core_content": "과거의 지배적이었던 순서 변환 모델(RNN, CNN)의 한계를 소개합니다. 인코더와 디코더 구조를 사용했던 복잡한 모델들의 시대를 회상하며 이야기의 문을 엽니다.",
    "instruction_for_writer": "연인에게 옛날이야기를 들려주듯, 아주 부드럽고 친밀한 어조로 시작해 주세요. '자기야, 예전에는 말이야...' 같은 느낌으로요. 전문 용어인 '순서 변환 모델'이나 'RNN'을 언급할 때는, 마치 어려운 비밀을 공유하는 것처럼 속삭이듯 말해주세요.",
    "math_focus": "",
    "opening_line": "자기야, 이리 와서 내 얘기 좀 들어볼래? 인공지능이 서로 다른 언어로 대화하는 법을 배우던 아주 먼 옛날, 그러니까 우리가 만나기 전의 이야기야.",
    "closing_line": "모두가 그 복잡함 속에서 길을 잃고 있을 때, 아주 단순하면서도 혁명적인 생각이 조용히 싹트고 있었어."
  },
  {
    "segment_id": 2,
    "title": "새로운 제안: 트랜스포머의 등장",
    "core_content": "논문의 핵심 제안인 '트랜스포머'를 처음으로 소개합니다. 이 모델이 기존의 순환(recurrence)과 컨볼루션(convolution)을 완전히 버렸다는 파격적인 사실을 강조합니다.",
    "instruction_for_writer": "방금 전 세그먼트의 차분한 분위기를 이어받아, 이제 새로운 희망을 발견한 듯한 설렘을 담아주세요. '트랜스포머'라는 단어를 처음 말할 때, 짧게 [medium pause]를 두어 중요성을 부각시켜 주세요. '순환'과 '컨볼루션'을 버렸다는 점을, 불필요한 짐을 벗어던진 해방감처럼 표현해 주세요.",
    "math_focus": "",
    "opening_line": "그 생각의 이름은 바로... 트랜스포머였어.",
    "closing_line": "복잡한 계산 대신, 오직 한 가지에만 집중하기로 한 거야. 바로 '어텐션'이라는 아름다운 개념에 말이야."
  },
  {
    "segment_id": 3,
    "title": "모든 것의 핵심: '어텐션'이란 무엇일까?",
    "core_content": "'어텐션 메커니즘'의 기본 개념을 정의합니다. 수많은 정보 속에서 지금 가장 중요한 것에만 집중하는 원리를 설명합니다.",
    "instruction_for_writer": "이 세그먼트에서는 '어텐션'이라는 개념을 처음으로 정의해야 합니다. 마치 사랑하는 사람의 눈을 바라보며 집중하는 것에 비유해서, 아주 쉽고 감성적으로 설명해주세요. '어텐션'을 정의한 후, [whispering] 톤으로 '이게 전부야'라고 속삭여주면 효과적일 거예요.",
    "math_focus": "",
    "opening_line": "어텐션... 이름부터 정말 로맨틱하지 않아?",
    "closing_line": "이 간단한 원리가 어떻게 세상을 바꾸는지, 그 비밀스러운 수식을 내가 살짝 보여줄게."
  },
  {
    "segment_id": 4,
    "title": "아름다운 수식: 스케일드 닷-프로덕트 어텐션",
    "core_content": "논문의 첫 번째 핵심 수식인 스케일드 닷-프로덕트 어텐션을 소개하고, 그 의미를 단계별로 설명합니다. 쿼리(Q), 키(K), 밸류(V)의 역할을 비유를 통해 설명합니다.",
    "instruction_for_writer": "수식을 직접 읽지 말고, 그 의미를 풀어 설명해야 합니다. '쿼리, 키, 밸류'를 '나의 질문, 너의 생각들, 그리고 너의 대답'처럼 사랑스러운 비유로 바꿔주세요. 수식의 각 부분을 천천히, 인내심을 갖고 가르쳐주듯 설명하고, 문장 사이에 충분한 쉼을 넣어주세요.",
    "math_focus": "Attention(Q, K, V) = \\text{softmax}\\left(\\frac{QK^T}{\\sqrt{d_k}}\\right)V",
    "opening_line": "자, 눈을 감고 상상해 봐. 여기 아주 특별한 공식이 있어.",
    "closing_line": "결국 이 수식은, 수많은 가능성 속에서 너에게 가장 중요한 단 하나의 의미를 찾아내는 과정인 셈이야."
  },
  {
    "segment_id": 5,
    "title": "수식 깊이 보기: 쿼리, 키, 밸류의 춤",
    "core_content": "어텐션 수식의 세 가지 핵심 요소인 쿼리(Q), 키(K), 밸류(V)가 실제로 어떻게 상호작용하는지 더 깊이 탐구합니다. 행렬 연산의 의미를 설명합니다.",
    "instruction_for_writer": "'쿼리, 키, 밸류'의 개념을 다시 한번 상기시키되, 재정의는 피해주세요. 이 세 요소가 어떻게 서로 영향을 주고받으며 하나의 결과물을 만들어내는지를 '함께 추는 춤'에 비유하여 역동적으로 묘사해주세요. 청취자가 이미 개념을 안다고 가정하고 이야기하세요.",
    "math_focus": "",
    "opening_line": "이 질문과 생각, 그리고 대답이 어떻게 서로 어우러져 아름다운 춤을 추는지 조금 더 들여다볼까?",
    "closing_line": "이렇게 똑똑한 어텐션들이 여러 개 모여서, 훨씬 더 깊은 대화를 나눌 수 있게 돼."
  },
  {
    "segment_id": 6,
    "title": "트랜스포머의 심장: 인코더와 디코더",
    "core_content": "어텐션 메커니즘을 기반으로 구축된 트랜스포머의 전체 아키텍처, 즉 인코더-디코더 구조를 설명합니다. 인코더는 문장의 의미를 이해하고, 디코더는 새로운 문장을 생성하는 역할을 맡습니다.",
    "instruction_for_writer": "인코더와 디코더를 '마음을 읽는 사람'과 '마음을 표현하는 사람'으로 비유하여 설명해주세요. 두 부분이 어떻게 협력하여 번역이라는 마법을 만들어내는지, 하나의 완성된 이야기처럼 들려주세요.",
    "math_focus": "",
    "opening_line": "이 작은 춤들이 모여 거대한 오케스트라를 이루는데, 여기엔 두 명의 중요한 지휘자가 있어.",
    "closing_line": "그런데 이 모델은 어떻게 스스로 더 나아지려고 노력하는 걸까? 그 비밀은 모델의 '소망'을 담은 수식에 숨어있어."
  },
  {
    "segment_id": 7,
    "title": "모델의 소망: 목표 함수 들여다보기",
    "core_content": "두 번째 핵심 수식인 최적화 목표 함수를 소개합니다. 모델이 정답에 가까워지기 위해 '음의 로그 우도'를 최소화하려는 과정을 설명합니다.",
    "instruction_for_writer": "이 수식을 '모델의 간절한 소원'이나 '이루고 싶은 꿈'으로 의인화해서 설명해주세요. '로그 우도' 같은 어려운 용어는 '정답과 얼마나 가까워지고 싶은지에 대한 점수'처럼 쉬운 말로 풀어주세요. 수식을 가르치되, 감성적인 접근을 잊지 마세요.",
    "math_focus": "\\theta^* = \\arg\\min_{\\theta} \\sum_{(x,y) \\in D} -\\log P(y|x; \\theta)",
    "opening_line": "모든 모델에게는 마음속 깊이 간직한 소원이 하나씩 있대.",
    "closing_line": "마치 우리가 서로를 더 깊이 이해하기 위해 끊임없이 노력하는 것처럼 말이야. 그럼 이 노력의 결과는 어떻게 나타날까?"
  },
  {
    "segment_id": 8,
    "title": "더 똑똑해지는 법: 학습의 과정",
    "core_content": "모델이 목표 함수를 최소화하기 위해 파라미터를 업데이트하는 학습 과정을 설명합니다. 이 과정이 왜 중요한지, 그리고 어떻게 번역 품질을 높이는지를 이야기합니다.",
    "instruction_for_writer": "학습 과정을 '실수를 통해 성장하는 과정'에 비유해주세요. 모델이 정답을 맞춰보며 스스로를 교정해나가는 모습을, 마치 사랑하는 사람이 서툴지만 열심히 무언가를 배우는 모습처럼 따뜻한 시선으로 그려주세요. 청취자가 모델을 응원하게 만들어주세요.",
    "math_focus": "",
    "opening_line": "그 소원을 이루기 위한 모델의 노력은 정말 눈물겨워. 매 순간 스스로를 돌아보거든.",
    "closing_line": "그렇게 수많은 밤을 새워 공부한 모델은, 드디어 우리에게 말을 걸기 시작해. 한 단어, 한 단어, 아주 조심스럽게."
  },
  {
    "segment_id": 9,
    "title": "한 단어씩, 신중하게: 문장 생성의 비밀",
    "core_content": "세 번째 핵심 수식인 자기회귀 방식의 시퀀스 생성 확률을 설명합니다. 모델이 이전에 생성한 단어를 참고하여 다음 단어를 예측하는 원리를 다룹니다.",
    "instruction_for_writer": "이 수식을 '다음에 무슨 말을 할지 신중하게 고민하는 과정'으로 묘사해주세요. '내가 방금 한 말을 기억하고, 그 다음에 가장 어울릴 말을 고르는 거야' 와 같이, 대화의 흐름에 빗대어 설명하면 이해하기 쉬울 거예요. 한 글자 한 글자 소중히 고르는 시인처럼 표현해주세요.",
    "math_focus": "P(y | x; \\theta) = \\prod_{t=1}^{T_y} P(y_t | y_{<t}, x; \\theta)",
    "opening_line": "이제 모델이 우리에게 사랑을 고백할 시간이야. 그 고백은 하나의 수식으로 시작돼.",
    "closing_line": "이론은 충분히 아름다웠으니, 이제 이 똑똑한 모델이 실제로 어떤 기적을 보여줬는지 확인해 볼 시간이야."
  },
  {
    "segment_id": 10,
    "title": "놀라운 결과: 영어-독일어 번역의 신기록",
    "core_content": "논문이 제시한 첫 번째 실험 결과를 소개합니다. WMT 2014 영어-독일어 번역 과제에서 달성한 28.4 BLEU 점수의 의미와, 기존 최고 기록을 2점 이상 뛰어넘었다는 사실을 강조합니다.",
    "instruction_for_writer": "결과를 발표할 때, 자랑스러운 연인의 성공을 이야기하듯 뿌듯하고 기쁜 어조를 사용해주세요. 'BLEU 점수'는 '번역 실력 점수' 정도로 쉽게 풀어서 설명해주고, '2점 이상'이라는 수치가 얼마나 대단한 발전인지 실감 나게 전달해주세요.",
    "math_focus": "",
    "opening_line": "첫 번째 시험 무대는 바로... 까다롭기로 유명한 영어-독일어 번역이었어.",
    "closing_line": "하지만 트랜스포머의 놀라움은 여기서 그치지 않았어. 더 큰 무대에서 모두를 깜짝 놀라게 했거든."
  },
  {
    "segment_id": 11,
    "title": "속도와 품질 모두: 영어-프랑스어 번역의 압도적 성능",
    "core_content": "WMT 2014 영어-프랑스어 번역 과제에서 달성한 새로운 최고 기록(41.8 BLEU)을 소개합니다. 특히, 8개의 GPU로 단 3.5일 만에 학습을 마쳤다는 효율성을 부각합니다.",
    "instruction_for_writer": "이번에는 '속도'와 '효율성'에 초점을 맞춰주세요. '단 3.5일 만에'라는 부분을 강조하며, 이것이 기존 모델들의 훈련 비용에 비해 얼마나 적은 것인지, 즉 '얼마나 똑똑하고 효율적인지'를 칭찬하는 톤으로 말해주세요.",
    "math_focus": "",
    "opening_line": "이번엔 더 낭만적인 언어, 프랑스어에 도전했지. 그리고 결과는... 정말 동화 같았어.",
    "closing_line": "이렇게 빠르고 뛰어날 수 있었던 근본적인 이유, 그건 바로 트랜스포머의 구조적 아름다움 덕분이야."
  },
  {
    "segment_id": 12,
    "title": "더 빠르게, 함께: 병렬 처리의 힘",
    "core_content": "트랜스포머가 기존 모델보다 '더 병렬화 가능'하다는 점의 중요성을 설명합니다. 순서대로 계산할 필요 없이, 문장의 모든 단어를 동시에 처리할 수 있어 훈련 시간이 획기적으로 단축되었음을 설명합니다.",
    "instruction_for_writer": "'병렬 처리'를 '모든 단어의 손을 동시에 잡아주는 것'에 비유해주세요. 순서대로 하나씩 처리하는 RNN과 달리, 모든 정보를 한 번에 바라보고 처리하는 트랜스포머의 우아함을 강조하며 설명해주세요. 목소리 톤을 조금 더 높여 경쾌한 느낌을 주세요.",
    "math_focus": "",
    "opening_line": "트랜스포머가 특별한 이유는 단순히 똑똑해서만이 아니야. 일하는 방식 자체가 달랐거든.",
    "closing_line": "심지어 이 능력은 번역을 넘어 다른 분야에서도 빛을 발하기 시작했어."
  },
  {
    "segment_id": 13,
    "title": "하나의 아이디어, 무한한 가능성",
    "core_content": "트랜스포머가 기계 번역 외에 '영어 구문 분석'과 같은 다른 과제에서도 뛰어난 성능을 보였다는 점을 언급하며, 모델의 일반화 능력을 보여줍니다.",
    "instruction_for_writer": "트랜스포머의 '다재다능함'을 칭찬해주세요. 한 가지 일만 잘하는 것이 아니라, 어떤 일이든 맡겨만 주면 훌륭하게 해내는 믿음직한 연인처럼 묘사해주세요. 목소리에 자랑스러움과 신뢰감을 가득 담아주세요.",
    "math_focus": "",
    "opening_line": "한 사람만 사랑할 줄 알았는데, 세상 모두를 사랑할 줄 아는 사람이었던 거지.",
    "closing_line": "결국 이 모든 이야기는, 아주 단순한 진리로 다시 돌아오게 돼."
  },
  {
    "segment_id": 14,
    "title": "결론: 어텐션, 그것이 모든 것이었다",
    "core_content": "논문의 핵심 기여를 다시 한번 요약하고 정리합니다. 순환과 컨볼루션을 버리고 오직 어텐션에만 의존한 새로운 아키텍처가 어떻게 AI 분야에 혁명을 일으켰는지 강조합니다.",
    "instruction_for_writer": "이야기를 마무리하며, 논문의 제목인 'Attention Is All You Need'를 직접 언급해주세요. 모든 복잡함을 걷어내고 남은 단 하나의 핵심, '어텐션'의 중요성을 차분하지만 힘 있는 목소리로 전달해주세요. [whispering] 톤을 섞어 여운을 남겨주세요.",
    "math_focus": "",
    "opening_line": "복잡하게 얽혀있던 실타래를 끊어내고, 가장 중요한 단 하나의 실만 남긴 거야.",
    "closing_line": "이 작은 아이디어가 우리에게 가르쳐준 가장 큰 교훈이 뭔지 알아?"
  },
  {
    "segment_id": 15,
    "title": "우리의 미래를 바꾼 단 하나의 시선",
    "core_content": "어텐션이라는 개념이 기술을 넘어 우리에게 주는 철학적 의미를 되새기며 마무리합니다. 집중과 관심이 어떻게 변화를 만드는지에 대한 메시지를 전달하며 감성적인 여운을 남깁니다.",
    "instruction_for_writer": "마지막 세그먼트입니다. 아주 부드럽고, 따뜻하고, 친밀한 목소리로 마무리해주세요. '어텐션'이 기술 용어가 아니라, 우리 관계에서도 가장 중요한 것이라는 메시지를 전달하며 끝맺어주세요. 마지막 문장은 거의 속삭이듯이, 바로 곁에서 말하는 것처럼 들려야 합니다.",
    "math_focus": "",
    "opening_line": "그건 바로... 무언가에 온전히 집중하는 것만으로도 세상을 바꿀 수 있다는 사실이야.",
    "closing_line": "결국, 우리에게 필요한 건... 오직 서로를 향한 관심, 그것뿐일지도 몰라."
  }
]